import pandas as pd
import numpy as np
import logging
import uuid
from datetime import datetime, timedelta
import os
from typing import Dict
from sklearn.linear_model import LinearRegression
from .decision_layer import ForecastDecisionLayer
from .learning_engine import LearningFeedbackEngine

# Import Django models
from django.forms.models import model_to_dict
from Produit.models import Produit, HistoriqueDemande, DelaisApprovisionnement, PolitiqueReapprovisionnement, cmd_achat_ouvertes_opt
from Transaction.models import Transaction, LigneTransaction
from warhouse.models import Stock, Emplacement, Entrepot

# Set up paths for reports
REPORT_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "reports")
os.makedirs(REPORT_DIR, exist_ok=True)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(os.path.join(REPORT_DIR, "forecasting_service.log")),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ForecastingService")

class DataLoader:
    def __init__(self, data_path=None, is_csv=False):
        self.data_path = data_path
        self.is_csv = is_csv
        self.demand_history = None
        self.transactions = None
        self.transaction_lines = None
        self.products = None
        self.stocks = None
        self.emplacements = None

    def load_and_clean(self):
        if (
            self.demand_history is not None
            and self.transaction_lines is not None
            and self.products is not None
            and self.transactions is not None
            and not self.demand_history.empty
        ):
            return
            
        if self.data_path is None:
            logger.info("No data path provided. Loading from Supabase (Django Models)...")
            self._load_from_django()
        else:
            logger.info(f"Loading data from {self.data_path} (is_csv={self.is_csv})")
            if not self.is_csv:
                # Load sheets from Excel
                xls = pd.ExcelFile(self.data_path)
                self.demand_history = pd.read_excel(xls, sheet_name='historique_demande')
                self.transaction_lines = pd.read_excel(xls, sheet_name='lignes_transaction')
                self.products = pd.read_excel(xls, sheet_name='produits')
                if 'transactions' in xls.sheet_names:
                    self.transactions = pd.read_excel(xls, sheet_name='transactions')
            else:
                # Load from CSV files in the directory
                self.demand_history = pd.read_csv(os.path.join(self.data_path, 'historique_demande.csv'))
                self.transaction_lines = pd.read_csv(os.path.join(self.data_path, 'lignes_transaction.csv'))
                self.products = pd.read_csv(os.path.join(self.data_path, 'produits.csv'))
                
                t_path = os.path.join(self.data_path, 'transactions.csv')
                if os.path.exists(t_path):
                    self.transactions = pd.read_csv(t_path)
                
                s_path = os.path.join(self.data_path, 'stocks.csv')
                if os.path.exists(s_path):
                    self.stocks = pd.read_csv(s_path)
                
                e_path = os.path.join(self.data_path, 'emplacements.csv')
                if os.path.exists(e_path):
                    self.emplacements = pd.read_csv(e_path)

        # Ensure column names are clean
        for df in [self.demand_history, self.transaction_lines, self.products, self.transactions, self.stocks, self.emplacements]:
            if df is not None and not df.empty:
                # Convert column names to strings first, then strip whitespace
                df.columns = [str(col).strip() for col in df.columns]

        self._normalize_forecasting_columns()

    def _normalize_forecasting_columns(self):
        """Normalize ORM FK column names to the schema expected by forecasting logic."""
        if self.demand_history is not None and not self.demand_history.empty:
            if 'id_produit' not in self.demand_history.columns and 'id_produit_id' in self.demand_history.columns:
                self.demand_history = self.demand_history.rename(columns={'id_produit_id': 'id_produit'})

        if self.transaction_lines is not None and not self.transaction_lines.empty:
            rename_map = {}
            if 'id_transaction' not in self.transaction_lines.columns and 'id_transaction_id' in self.transaction_lines.columns:
                rename_map['id_transaction_id'] = 'id_transaction'
            if 'id_produit' not in self.transaction_lines.columns and 'id_produit_id' in self.transaction_lines.columns:
                rename_map['id_produit_id'] = 'id_produit'
            if rename_map:
                self.transaction_lines = self.transaction_lines.rename(columns=rename_map)

        self._normalize_id_types()

    def _normalize_id_types(self):
        """Force consistent dtypes for keys used in joins and filters."""
        for attr_name in ['demand_history', 'transaction_lines', 'products']:
            df = getattr(self, attr_name, None)
            if df is None or df.empty or 'id_produit' not in df.columns:
                continue

            numeric_ids = pd.to_numeric(df['id_produit'], errors='coerce')
            df = df.loc[numeric_ids.notna()].copy()
            df['id_produit'] = numeric_ids.loc[numeric_ids.notna()].astype(int)
            setattr(self, attr_name, df)

        if self.transactions is not None and not self.transactions.empty and 'id_transaction' in self.transactions.columns:
            self.transactions['id_transaction'] = self.transactions['id_transaction'].astype(str).str.strip()

        if self.transaction_lines is not None and not self.transaction_lines.empty and 'id_transaction' in self.transaction_lines.columns:
            self.transaction_lines['id_transaction'] = self.transaction_lines['id_transaction'].astype(str).str.strip()

    def _load_from_django(self):
        """Fetches data directly from Supabase via Django ORM."""
        try:
            # 1. Products
            prods = Produit.objects.all().values()
            self.products = pd.DataFrame(list(prods))
            
            # 2. Demand History
            history = HistoriqueDemande.objects.all().values()
            self.demand_history = pd.DataFrame(list(history))
            
            # 3. Transactions & Lines - Now safe to use .values() with db_column fix
            transactions = Transaction.objects.all().values(
                'id_transaction', 'type_transaction', 'reference_transaction', 
                'cree_le', 'statut', 'notes'
            )
            self.transactions = pd.DataFrame(list(transactions))
            
            lines = LigneTransaction.objects.all().values()
            self.transaction_lines = pd.DataFrame(list(lines))
            
            # 4. Stocks (Initial Quantities)
            stocks = Stock.objects.all().values()
            self.stocks = pd.DataFrame(list(stocks))
            
            # 5. Emplacements (Occupancy State)
            emplacements = Emplacement.objects.all().values()
            self.emplacements = pd.DataFrame(list(emplacements))
            
            logger.info(f"Successfully fetched {len(self.products)} products and {len(self.demand_history)} history points from Supabase.")
        except Exception as e:
            logger.error(f"Error fetching data from Supabase: {e}")
            # Fallback to empty DataFrames to avoid crash
            self.products = pd.DataFrame()
            self.demand_history = pd.DataFrame()
            self.transactions = pd.DataFrame()
            self.transaction_lines = pd.DataFrame()
            self.stocks = pd.DataFrame()
            self.emplacements = pd.DataFrame()

    def load_and_clean_wrapper(self):
        """Main entry point for loading and cleaning."""
        self.load_and_clean()
        self._normalize_forecasting_columns()
        
        # Clean Demand History
        if self.demand_history is not None and not self.demand_history.empty:
            if 'date' in self.demand_history.columns and 'quantite_demande' in self.demand_history.columns and 'id_produit' in self.demand_history.columns:
                self.demand_history['date'] = pd.to_datetime(self.demand_history['date'], errors='coerce')
                self.demand_history['quantite_demande'] = pd.to_numeric(self.demand_history['quantite_demande'], errors='coerce')
                self.demand_history = self.demand_history.dropna(subset=['date', 'id_produit', 'quantite_demande'])
                self.demand_history = self.demand_history[self.demand_history['quantite_demande'] >= 0]
                self.demand_history['date'] = self.demand_history['date'].dt.normalize()
                self.demand_history = self.demand_history.groupby(['id_produit', 'date'], as_index=False)['quantite_demande'].sum()
                self.demand_history = self.demand_history.sort_values(['id_produit', 'date'])

        # Clean Products
        if self.products is not None and not self.products.empty:
            self.products = self._clean_df(self.products)
        
        # Clean Transactions and Lines
        if self.transactions is not None and not self.transactions.empty:
            self.transactions = self._clean_df(self.transactions, 'cree_le')
        if self.transaction_lines is not None and not self.transaction_lines.empty:
            self.transaction_lines = self._clean_df(self.transaction_lines)
            
        logger.info("Data workflow complete.")

    def _clean_df(self, df, date_col=None):
        # Remove template/dummy rows (commonly 'texte' or 'O')
        if not df.empty and len(df.columns) > 0:
            # Assuming 'id' columns or first columns might have 'texte'
            first_col = df.columns[0]
            # Convert to string safely, handling any non-string types
            try:
                df_copy = df.copy()
                df_copy[first_col] = df_copy[first_col].fillna('').astype(str)
                df = df[~df_copy[first_col].str.contains('texte|identifiant|entier|^O$', case=False, na=False)]
            except Exception as e:
                logger.warning(f"Could not clean DataFrame using first column: {e}")
        
        if date_col and date_col in df.columns:
            df[date_col] = pd.to_datetime(df[date_col], errors='coerce')
            df = df.dropna(subset=[date_col])
        
        return df.drop_duplicates()

    def get_current_stock(self):
        """
        Step 1 ‚Äî Compute Current Stock from transactions.
        Stock per SKU = Receipts - Transfers - Picking - Deliveries
        """
        if hasattr(self, '_cached_stock') and self._cached_stock is not None:
            return self._cached_stock

        logger.info("Computing current stock levels from transaction history.")
        
        # Check if we have data to work with
        if self.transaction_lines is None or self.transaction_lines.empty:
            logger.warning("No transaction lines available for stock calculation.")
            self._cached_stock = {}
            return self._cached_stock
            
        if self.transactions is None or self.transactions.empty:
            logger.warning("No transactions available for stock calculation.")
            self._cached_stock = {}
            return self._cached_stock
        
        movements = pd.merge(self.transaction_lines, self.transactions, on='id_transaction', how='inner')
        
        if movements.empty:
            logger.warning("No movements found after merging transactions and lines.")
            self._cached_stock = {}
            return self._cached_stock
        
        # Chronological Integrity Fix: Ensure movements are sorted by date
        date_col = 'cree_le' if 'cree_le' in movements.columns else None
        if not date_col:
            for col in movements.columns:
                if 'date' in col.lower() or 'cree' in col.lower():
                    date_col = col
                    break
        
        if date_col:
            movements[date_col] = pd.to_datetime(movements[date_col], errors='coerce')
            movements = movements.sort_values(date_col).dropna(subset=[date_col])
        
        movements['qty_numeric'] = pd.to_numeric(movements['quantite'], errors='coerce').fillna(0)
        movements['type_transaction'] = movements['type_transaction'].astype(str).str.strip().fillna('UNKNOWN')
        
        # Defined multipliers for chronological flow
        multipliers = {
            'RECEIPT': 1, 
            'DELIVERY': -1, 
            'PICKING': -1, 
            'ISSUE': -1, 
            'TRANSFER': -1, 
            'ADJUSTMENT': 1
        }
        
        movements['net_change'] = (movements['qty_numeric'] * 
                                  movements['type_transaction'].str.upper().map(multipliers).fillna(0))
        
        # üü¢ Requirement FIX: Initialize stock from the Stock snapshot if available
        sku_stock = {}
        if self.loader.stocks is not None and not self.loader.stocks.empty:
            if 'id_produit_id' in self.loader.stocks.columns:
                self.loader.stocks = self.loader.stocks.rename(columns={'id_produit_id': 'id_produit'})
            
            if 'id_produit' in self.loader.stocks.columns and 'quantite' in self.loader.stocks.columns:
                # Group by product and sum up initial stocks across all locations
                initial_stocks = self.loader.stocks.groupby('id_produit')['quantite'].sum().to_dict()
                for pid, qty in initial_stocks.items():
                    try:
                        sku_stock[int(pid)] = float(qty)
                    except ValueError:
                        continue
                logger.info(f"Initialized stock for {len(sku_stock)} SKUs from physical inventory snapshot.")

        violations = 0
        
        for _, row in movements.iterrows():
            pid = row['id_produit']
            change = float(row['net_change'])
            
            if pid not in sku_stock:
                sku_stock[pid] = 0.0
            
            if sku_stock[pid] + change < 0:
                violations += 1
                # Chronological Integrity: Reject/Clip negative stock but track it
                sku_stock[pid] = 0.0
            else:
                sku_stock[pid] += change
        
        if violations > 0:
            logger.warning(f"Chronological Violation: {violations} operations attempted to reduce stock below zero.")
            
        self._cached_stock = sku_stock
        
        logger.info(f"Calculated stock for {len(self._cached_stock)} SKUs with chronological validation.")
        return self._cached_stock

class BaselineModel:
    """1Ô∏è‚É£ Simple Moving Average (Baseline): Predict next-day demand using last 7 days average."""
    def predict(self, history, product_id, window=7):
        product_history = history[history['id_produit'] == product_id]
        if product_history.empty:
            return 0
        # Average demand of the last N days
        return product_history['quantite_demande'].tail(window).mean()

class RegressionModel:
    """2Ô∏è‚É£ Linear Regression Forecast: Fit line on past demand and extract trend/prediction."""
    def __init__(self):
        from sklearn.linear_model import LinearRegression
        self.model = LinearRegression()

    def analyze(self, history, product_id):
        product_history = history[history['id_produit'] == product_id].copy()
        if len(product_history) < 3:
            return {
                'prediction': 0,
                'slope': 0,
                'trend': 'stable',
                'std_dev': 0,
                'volatility': 'stable',
                'safety_stock': 0,
                'trend_strength': 0,
                'trend_significant': False
            }
            
        # Convert dates to numeric (days since first date)
        first_date = product_history['date'].min()
        X = (product_history['date'] - first_date).dt.days.values.reshape(-1, 1)
        y = product_history['quantite_demande'].values
        
        self.model.fit(X, y)
        r2_score = float(self.model.score(X, y)) if len(y) >= 3 else 0.0
        
        slope = self.model.coef_[0]
        next_day_num = (product_history['date'].max() - first_date).days + 1
        prediction = max(0, self.model.predict([[next_day_num]])[0])
        
        # 3Ô∏è‚É£ Compute Volatility (Standard Deviation)
        std_dev = float(product_history['quantite_demande'].std()) if len(product_history) > 1 else 0.0
        if np.isnan(std_dev):
            std_dev = 0.0

        if std_dev < 10:
            volatility = "stable"
        else:
            volatility = "high fluctuation"

        # 4Ô∏è‚É£ Compute Safety Stock (95% Service Level)
        safety_stock = 1.65 * std_dev

        # Trend significance: only trust regression when slope is meaningful vs mean demand
        mean_demand = float(product_history['quantite_demande'].mean())
        demand_scale = max(mean_demand, 1.0)
        trend_strength = abs(float(slope)) / demand_scale
        # Lowered thresholds for better WAP: R¬≤>=0.05 (was 0.10), 7 days (was 14)
        trend_significant = (trend_strength >= 0.001) and (r2_score >= 0.05) and (len(product_history) >= 7)

        if trend_significant and slope > 0:
            trend = "increasing"
        elif trend_significant and slope < 0:
            trend = "decreasing"
        else:
            trend = "stable"
            
        return {
            'prediction': float(prediction),
            'slope': float(slope),
            'trend': trend,
            'std_dev': float(std_dev),
            'volatility': volatility,
            'safety_stock': float(safety_stock),
            'trend_strength': float(trend_strength),
            'trend_significant': bool(trend_significant),
            'r2_score': float(r2_score)
        }

class DeterministicForecastModel:
    """
    Inventory-oriented deterministic forecaster using:
    - Exponential Smoothing (SES)
    - Guarded Linear Regression (only when trend is significant)
    - Optimized for WAP < 30% and Bias 0-5%
    """
    def __init__(self):
        self.default_alpha_fast = 0.40  # Increased from 0.35 for faster response
        self.default_alpha_slow = 0.25  # Increased from 0.20

    def _prepare_series(self, history, product_id):
        product_history = history[history['id_produit'] == product_id].sort_values('date').copy()
        if product_history.empty:
            return product_history

        # Keep observed transaction days only (avoid creating artificial zero-demand days)
        product_history['quantite_demande'] = pd.to_numeric(product_history['quantite_demande'], errors='coerce').fillna(0.0)

        # Outlier handling (IQR clipping) on historical values only
        q1 = product_history['quantite_demande'].quantile(0.25)
        q3 = product_history['quantite_demande'].quantile(0.75)
        iqr = q3 - q1
        lower = max(0.0, q1 - 1.5 * iqr)
        upper = q3 + 1.5 * iqr if iqr > 0 else max(q3, 0.0)
        product_history['quantite_demande'] = product_history['quantite_demande'].clip(lower=lower, upper=upper)

        # Zero-demand smoothing for fast movers only (keeps intermittents untouched)
        avg = float(product_history['quantite_demande'].mean())
        zero_ratio = float((product_history['quantite_demande'] == 0).mean())
        if avg >= 5 and zero_ratio < 0.4:
            roll3 = product_history['quantite_demande'].rolling(3, min_periods=1).mean()
            mask_zero = product_history['quantite_demande'] == 0
            product_history.loc[mask_zero, 'quantite_demande'] = roll3[mask_zero] * 0.35

        return product_history

    def classify_demand(self, series):
        if len(series) == 0:
            return 'slow_mover'
        mean_demand = float(series.mean())
        zero_ratio = float((series == 0).mean())
        cv = float(series.std() / max(mean_demand, 1.0))

        if zero_ratio >= 0.5 or mean_demand < 3:
            return 'slow_mover'
        if cv > 1.2:
            return 'volatile_fast'
        return 'fast_mover'

    def weighted_moving_average(self, series, window=7):
        values = series.tail(window).values
        if len(values) == 0:
            return 0.0
        weights = np.arange(1, len(values) + 1, dtype=float)
        return float(np.dot(values, weights) / weights.sum())

    def simple_exponential_smoothing(self, series, alpha=0.3):
        values = series.values
        if len(values) == 0:
            return 0.0
        smoothed = float(values[0])
        for val in values[1:]:
            smoothed = alpha * float(val) + (1 - alpha) * smoothed
        return max(0.0, smoothed)

    def compute_safety_stock(self, series, demand_class):
        std_dev = float(series.std()) if len(series) > 1 else 0.0
        if np.isnan(std_dev):
            std_dev = 0.0
        if demand_class == 'volatile_fast':
            z = 1.65
        elif demand_class == 'fast_mover':
            z = 1.28
        else:
            z = 1.0
        return float(max(0.0, z * std_dev))

    def calculate_confidence(self, series, demand_class, has_seasonality):
        """
        Calculates a confidence score (0-100) based on data volume and stability.
        """
        if len(series) == 0: return 0
        
        # 1. Volume Score (0-40 points)
        volume_score = min(40, len(series)) # Max 40 points if 40+ days of history
        
        # 2. Stability Score (0-40 points)
        cv = series.std() / series.mean() if series.mean() > 0 else 2.0
        stability_score = max(0, 40 - (cv * 20))
        
        # 3. Pattern Bonus (0-20 points)
        bonus = 20 if has_seasonality else 0
        
        total = volume_score + stability_score + bonus
        return int(min(100, max(10, total)))

    def get_yoy_seasonal_demand(self, history, product_id, target_date=None):
        """
        Year-over-Year Seasonality: Check demand from the same day/month in previous years.
        This captures annual patterns (holidays, seasonal effects, etc.)
        """
        product_history = history[history['id_produit'] == product_id].copy()
        if product_history.empty:
            return {'yoy_avg': 0.0, 'yoy_count': 0, 'has_pattern': False}
        
        # Use today if no target date specified
        if target_date is None:
            target_date = datetime.now()
        
        target_day = target_date.day
        target_month = target_date.month
        
        # Find all records matching same day/month from previous years
        product_history['day'] = product_history['date'].dt.day
        product_history['month'] = product_history['date'].dt.month
        product_history['year'] = product_history['date'].dt.year
        
        yoy_matches = product_history[
            (product_history['day'] == target_day) & 
            (product_history['month'] == target_month) &
            (product_history['year'] < target_date.year)
        ]
        
        if len(yoy_matches) > 0:
            yoy_avg = float(yoy_matches['quantite_demande'].mean())
            yoy_count = len(yoy_matches)
            has_pattern = yoy_count >= 2  # At least 2 years of history
            
            logger.debug(f"YoY Seasonality for SKU {product_id}: Found {yoy_count} matches "
                        f"for {target_month}/{target_day}, avg demand: {yoy_avg:.2f}")
            
            return {
                'yoy_avg': yoy_avg,
                'yoy_count': yoy_count,
                'has_pattern': has_pattern
            }
        
        return {'yoy_avg': 0.0, 'yoy_count': 0, 'has_pattern': False}

    def predict(self, history, product_id, regression_model, target_date=None, learning_engine=None):
        prepared = self._prepare_series(history, product_id)
        if prepared is None or prepared.empty or prepared['quantite_demande'].sum() == 0:
            return {
                'forecast': 1.0 if not prepared.empty else 0.0, # Baseline 1 for active but zero-demand SKUs
                'ses': 0.0,
                'regression': 0.0,
                'confidence': 10 if not prepared.empty else 0,
                'yoy_seasonal': 0.0,
                'trend': 'stable',
                'volatility': 'stable',
                'trend_strength': 0.0,
                'demand_class': 'slow_mover',
                'safety_stock': 0.0,
                'reasoning': 'Active SKU but zero demand in history.' if not prepared.empty else 'No history available.'
            }

        series = prepared['quantite_demande'].astype(float)
        demand_class = self.classify_demand(series)

        alpha = self.default_alpha_fast if demand_class != 'slow_mover' else self.default_alpha_slow
        ses = self.simple_exponential_smoothing(series, alpha=alpha)

        reg_results = regression_model.analyze(prepared, product_id)
        reg_pred = float(reg_results.get('prediction', 0.0))
        trend = reg_results.get('trend', 'stable')
        trend_strength = float(reg_results.get('trend_strength', 0.0))
        trend_significant = bool(reg_results.get('trend_significant', False))
        volatility = reg_results.get('volatility', 'stable')

        # Year-over-Year Seasonality Check
        yoy_data = self.get_yoy_seasonal_demand(history, product_id, target_date)
        yoy_seasonal = yoy_data['yoy_avg']
        has_seasonal_pattern = yoy_data['has_pattern']

        # Optimized for WAP < 30%, Bias 0-5%: Pure REG strategy with minimal SES
        if has_seasonal_pattern and yoy_seasonal > 0:
            # If we have strong seasonal pattern (2+ years), blend it in
            if demand_class == 'slow_mover':
                forecast = 0.20 * ses + 0.55 * reg_pred + 0.25 * yoy_seasonal
                rationale = f'Slow mover with YoY: REG-primary (YoY avg: {yoy_seasonal:.1f}).'
            elif trend_significant:
                forecast = 0.05 * ses + 0.80 * reg_pred + 0.15 * yoy_seasonal
                rationale = f'Strong trend & YoY: REG-dominant (YoY avg: {yoy_seasonal:.1f}).'
            else:
                forecast = 0.15 * ses + 0.65 * reg_pred + 0.20 * yoy_seasonal
                rationale = f'Fast mover with YoY: REG-heavy (YoY avg: {yoy_seasonal:.1f}).'
        else:
            # No seasonality: Near-pure REG for best WAP
            if demand_class == 'slow_mover':
                forecast = 0.25 * ses + 0.75 * reg_pred
                rationale = 'Slow mover: REG-dominant for accuracy.'
            elif trend_significant:
                forecast = 0.05 * ses + 0.95 * reg_pred
                rationale = 'Significant trend: Near-pure REG (optimal WAP).'
            else:
                # Default: heavily favor regression
                forecast = 0.15 * ses + 0.85 * reg_pred
                rationale = 'Stable demand: REG-primary strategy.'

        safety_stock = self.compute_safety_stock(series, demand_class)
        confidence = self.calculate_confidence(series, demand_class, has_seasonal_pattern)
        
        # Adaptive calibration: Using the learning engine to adjust over time
        if learning_engine:
            factor = learning_engine.get_calibration_factor(product_id)
            if abs(factor - 1.27) > 0.01:
                rationale += f" [Adaptive Feedback: {factor:.2f}x]"
        else:
            factor = 1.27
            
        forecast = float(max(0.0, forecast * factor))

        return {
            'forecast': forecast,
            'ses': float(ses),
            'regression': float(reg_pred),
            'confidence': confidence,
            'yoy_seasonal': float(yoy_seasonal),
            'yoy_pattern_detected': has_seasonal_pattern,
            'trend': trend,
            'volatility': volatility,
            'trend_strength': float(trend_strength),
            'demand_class': demand_class,
            'safety_stock': float(safety_stock),
            'reasoning': rationale
        }

class PreparationOrderService:
    def generate_order_advanced(self, forecast_data, current_stock):
        """
        Step 3 ‚Äî Create Preparation Order Object
        Structure: Order ID, Date, List of SKU + quantity, Status, Source
        """
        items = []
        for product_id, data in forecast_data.items():
            stock = current_stock.get(product_id, 0)
            forecast = data.get('forecast', 0.0) # Use the calibrated forecast
            
            # Step 2: Compute Preparation Quantity
            # If we expect to ship X tomorrow, and we have Y in picking area,
            # we need to prepare X - Y if X > Y.
            prep_qty = max(0, forecast - stock)
            
            if prep_qty > 0:
                items.append({
                    'sku_id': product_id,
                    'quantity': round(prep_qty, 2),
                    'target_forecast': round(forecast, 2),
                    'current_stock': stock,
                    'confidence': data.get('confidence', 0),
                    'reasoning': data.get('reasoning', '')
                })
        
        if not items:
            return None

        order_id = f"PREP-{uuid.uuid4().hex[:8].upper()}"
        order_object = {
            'order_id': order_id,
            'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'items': items,
            'status': 'PENDING_SUPERVISOR_VALIDATION',
            'source': 'AI_FORECASTING_SERVICE'
        }
        return order_object

    def validate_order(self, order_obj, sku_id=None, override_qty=None, justification=None, learning_engine=None):
        """
        Step 1 ‚Äî If Approved / Step 2 ‚Äî If Overridden
        Implements auditability for manual interventions.
        """
        if order_obj is None: 
            logger.error("No order object provided for validation.")
            return None
        
        # Scenario: Supervisor performs an override
        if sku_id is not None:
            if override_qty is not None and not justification:
                logger.error(f"VAL-FAIL: Justification is required for manual override of SKU {sku_id}")
                return order_obj 
            
            for item in order_obj['items']:
                if item['sku_id'] == sku_id:
                    if override_qty is not None:
                        # Step 2: Save original, new value, and justification
                        item['original_ai_forecast'] = item['quantity']
                        item['new_manual_value'] = override_qty
                        item['quantity'] = override_qty # Active quantity for WMS
                        item['justification'] = justification
                        item['modified_by'] = "SUPERVISOR"
                        
                        # Log override permanently (Auditability)
                        logger.warning(f"AUDIT-LOG | Order: {order_obj['order_id']} | SKU: {sku_id} | "
                                       f"Changed from {item['original_ai_forecast']} to {override_qty} | "
                                       f"Reason: {justification}")
                        
                        # --- Model Learning: Supervisor Correction ---
                        if learning_engine:
                            # If supervisor reduces qty, AI was over-forecasting
                            learning_engine.update_with_actuals(sku_id, item['original_ai_forecast'], override_qty)
                    break
            
        # Step 1: If Approved (Status -> VALIDATED)
        order_obj['status'] = 'VALIDATED'
        order_obj['validation_date'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"ORDER {order_obj['order_id']} VALIDATED - Status changed to VALIDATED (Visible to Employees)")
        return order_obj

class ForecastingService:
    def __init__(self, data_path=None, is_csv=False):
        self.loader = DataLoader(data_path, is_csv=is_csv)
        self.baseline = BaselineModel()
        self.regression = RegressionModel()
        self.learning_engine = LearningFeedbackEngine(
            storage_path=os.path.join(REPORT_DIR, "model_learning.json")
        )
        self.deterministic = DeterministicForecastModel()
        self.decision_layer = ForecastDecisionLayer()
        self.order_service = PreparationOrderService()

    def _compute_model_validation_wape(self, history: pd.DataFrame, pid: int, horizon_points: int = 14) -> Dict[str, float]:
        """
        Computes SKU-level validation WAPE for SES, REG, and HYBRID using rolling one-step backtest.
        HYBRID uses inverse-WAPE blending derived from SES/REG performance.
        """
        if history is None or history.empty:
            return {
                'wape_ses': 100.0,
                'wape_reg': 100.0,
                'wape_hybrid': 100.0,
                'hybrid_weight_ses': 0.5,
                'hybrid_weight_reg': 0.5,
                'error_var_ses': 0.0,
                'error_var_reg': 0.0,
                'error_var_hybrid': 0.0,
            }

        local = history.sort_values('date').reset_index(drop=True)
        if len(local) < 12:
            return {
                'wape_ses': 100.0,
                'wape_reg': 100.0,
                'wape_hybrid': 100.0,
                'hybrid_weight_ses': 0.5,
                'hybrid_weight_reg': 0.5,
                'error_var_ses': 0.0,
                'error_var_reg': 0.0,
                'error_var_hybrid': 0.0,
            }

        start_idx = max(7, len(local) - max(5, int(horizon_points)))
        indices = list(range(start_idx, len(local)))

        ses_errors = []
        reg_errors = []
        ses_abs_sum = 0.0
        reg_abs_sum = 0.0
        actual_sum = 0.0

        for idx in indices:
            train = local.iloc[:idx].copy()
            actual = float(local.iloc[idx]['quantite_demande'])
            if train.empty:
                continue

            ses_pred = float(self.deterministic.simple_exponential_smoothing(
                train['quantite_demande'].astype(float),
                alpha=self.deterministic.default_alpha_fast
            ))
            reg_pred = float(self.regression.analyze(train, pid).get('prediction', ses_pred))

            ses_error = ses_pred - actual
            reg_error = reg_pred - actual

            ses_errors.append(ses_error)
            reg_errors.append(reg_error)
            ses_abs_sum += abs(ses_error)
            reg_abs_sum += abs(reg_error)
            actual_sum += abs(actual)

        if actual_sum <= 0:
            wape_ses = 100.0
            wape_reg = 100.0
        else:
            wape_ses = (ses_abs_sum / actual_sum) * 100.0
            wape_reg = (reg_abs_sum / actual_sum) * 100.0

        inv_ses = 1.0 / max(wape_ses, 1e-6)
        inv_reg = 1.0 / max(wape_reg, 1e-6)
        hybrid_weight_ses = float(inv_ses / (inv_ses + inv_reg))
        hybrid_weight_reg = float(inv_reg / (inv_ses + inv_reg))

        hybrid_errors = [
            (hybrid_weight_ses * se) + (hybrid_weight_reg * re)
            for se, re in zip(ses_errors, reg_errors)
        ]
        hybrid_abs_sum = float(sum(abs(e) for e in hybrid_errors))
        wape_hybrid = (hybrid_abs_sum / actual_sum) * 100.0 if actual_sum > 0 else 100.0

        return {
            'wape_ses': float(round(wape_ses, 4)),
            'wape_reg': float(round(wape_reg, 4)),
            'wape_hybrid': float(round(wape_hybrid, 4)),
            'hybrid_weight_ses': float(hybrid_weight_ses),
            'hybrid_weight_reg': float(hybrid_weight_reg),
            'error_var_ses': float(np.var(ses_errors)) if ses_errors else 0.0,
            'error_var_reg': float(np.var(reg_errors)) if reg_errors else 0.0,
            'error_var_hybrid': float(np.var(hybrid_errors)) if hybrid_errors else 0.0,
        }

    def _compute_dynamic_confidence(
        self,
        series: pd.Series,
        ses_pred: float,
        reg_pred: float,
        selected_error_var: float,
    ) -> int:
        """
        Dynamic confidence in [0, 100] using:
        - volatility (std/mean)
        - model agreement (SES vs REG distance)
        - historical error variance
        """
        mean_demand = float(max(series.mean(), 1.0))
        std_demand = float(series.std()) if len(series) > 1 else 0.0
        cv = std_demand / mean_demand

        model_gap_ratio = abs(float(ses_pred) - float(reg_pred)) / mean_demand
        error_std_ratio = (float(np.sqrt(max(selected_error_var, 0.0))) / mean_demand)

        volatility_component = 1.0 - min(cv / 2.0, 1.0)
        agreement_component = 1.0 - min(model_gap_ratio / 1.0, 1.0)
        error_component = 1.0 - min(error_std_ratio / 1.0, 1.0)

        score = (
            0.40 * volatility_component +
            0.30 * agreement_component +
            0.30 * error_component
        ) * 100.0

        return int(round(max(0.0, min(100.0, score))))

    def _select_and_compute_forecast(self, pid: int, history: pd.DataFrame, target_date=None) -> Dict:
        """
        Internal deterministic decision engine:
        - Selects winner by lowest SKU-level validation WAPE (SES/REG/HYBRID)
        - Applies transparent adjustment factor to base prediction
        - Enforces guardrails and returns explainable trace
        """
        prepared = self.deterministic._prepare_series(history, pid)
        if prepared is None or prepared.empty:
            return {
                'selected_model': 'SMA',
                'ses_pred': 0.0,
                'reg_pred': 0.0,
                'base_prediction': 0.0,
                'wape_ses': 100.0,
                'wape_reg': 100.0,
                'wape_hybrid': 100.0,
                'adjustment_factor': 1.0,
                'adjustment_detail': {'trend_multiplier': 1.0, 'bias_correction': 1.0, 'safety_factor': 1.0},
                'raw_forecast': 0.0,
                'final_forecast': 0.0,
                'confidence': 0,
                'justification': 'No history available. Conservative zero forecast.',
                'formula': '0.00 √ó 1.00 = 0.00',
            }

        series = prepared['quantite_demande'].astype(float)
        if series.sum() <= 0:
            return {
                'selected_model': 'SMA',
                'ses_pred': 0.0,
                'reg_pred': 0.0,
                'base_prediction': 1.0,
                'wape_ses': 100.0,
                'wape_reg': 100.0,
                'wape_hybrid': 100.0,
                'adjustment_factor': 1.0,
                'adjustment_detail': {'trend_multiplier': 1.0, 'bias_correction': 1.0, 'safety_factor': 1.0},
                'raw_forecast': 1.0,
                'final_forecast': 1.0,
                'confidence': 5,
                'justification': 'Sparse/zero demand history. Minimal non-negative baseline used.',
                'formula': '1.00 √ó 1.00 = 1.00',
            }

        demand_class = self.deterministic.classify_demand(series)
        alpha = self.deterministic.default_alpha_fast if demand_class != 'slow_mover' else self.deterministic.default_alpha_slow
        ses_pred = float(self.deterministic.simple_exponential_smoothing(series, alpha=alpha))
        reg_results = self.regression.analyze(prepared, pid)
        reg_pred = float(reg_results.get('prediction', ses_pred))

        validation = self._compute_model_validation_wape(prepared, int(pid))
        wape_map = {
            'SMA': validation['wape_ses'],
            'REG': validation['wape_reg'],
            'HYBRID': validation['wape_hybrid'],
        }
        selected_model = min(wape_map, key=wape_map.get)

        if selected_model == 'SMA':
            base_prediction = ses_pred
            selected_error_var = validation['error_var_ses']
            blend_formula = None
        elif selected_model == 'REG':
            base_prediction = reg_pred
            selected_error_var = validation['error_var_reg']
            blend_formula = None
        else:
            ws = validation['hybrid_weight_ses']
            wr = validation['hybrid_weight_reg']
            base_prediction = (ws * ses_pred) + (wr * reg_pred)
            selected_error_var = validation['error_var_hybrid']
            blend_formula = f"({ws:.3f}√óSES + {wr:.3f}√óREG)"

        trend_multiplier = 1.0
        trend_significant = bool(reg_results.get('trend_significant', False))
        slope = float(reg_results.get('slope', 0.0))
        mean_demand = float(max(series.mean(), 1.0))
        if trend_significant:
            trend_ratio = slope / mean_demand
            trend_multiplier = float(np.clip(1.0 + trend_ratio, 0.90, 1.15))

        bias_correction = float(np.clip(self.learning_engine.get_calibration_factor(pid), 0.85, 1.15))

        std_dev = float(series.std()) if len(series) > 1 else 0.0
        cv = std_dev / mean_demand
        safety_factor = float(np.clip(1.0 + (0.06 * min(cv, 1.0)), 1.0, 1.06))

        adjustment_factor = float(trend_multiplier * bias_correction * safety_factor)
        raw_forecast = float(base_prediction * adjustment_factor)

        hist_max = float(series.max()) if len(series) > 0 else 0.0
        sparse_multiplier = 1.15 if len(series) < 14 else 1.25
        cap_value = max(1.0, hist_max * sparse_multiplier)

        guardrailed = max(0.0, raw_forecast)
        guardrailed = min(guardrailed, cap_value)

        confidence = self._compute_dynamic_confidence(
            series=series,
            ses_pred=ses_pred,
            reg_pred=reg_pred,
            selected_error_var=selected_error_var,
        )
        if len(series) < 14:
            confidence = max(0, min(confidence, 55))

        if selected_model == 'HYBRID':
            formula = (
                f"{blend_formula} = {base_prediction:.2f}; "
                f"Final = {base_prediction:.2f} √ó {adjustment_factor:.4f} = {raw_forecast:.2f}; "
                f"GuardrailCap={cap_value:.2f} -> {guardrailed:.2f}"
            )
            justification = (
                "HYBRID selected due to lowest validation WAPE. "
                "Adjustment combines trend, calibration bias, and volatility safety factor."
            )
        else:
            formula = (
                f"Final = {base_prediction:.2f} √ó {adjustment_factor:.4f} = {raw_forecast:.2f}; "
                f"GuardrailCap={cap_value:.2f} -> {guardrailed:.2f}"
            )
            justification = (
                f"{selected_model} selected due to lowest validation WAPE. "
                "Adjustment combines trend, calibration bias, and volatility safety factor."
            )

        return {
            'selected_model': selected_model,
            'ses_pred': float(ses_pred),
            'reg_pred': float(reg_pred),
            'base_prediction': float(base_prediction),
            'wape_ses': float(validation['wape_ses']),
            'wape_reg': float(validation['wape_reg']),
            'wape_hybrid': float(validation['wape_hybrid']),
            'adjustment_factor': adjustment_factor,
            'adjustment_detail': {
                'trend_multiplier': trend_multiplier,
                'bias_correction': bias_correction,
                'safety_factor': safety_factor,
            },
            'raw_forecast': float(raw_forecast),
            'final_forecast': float(round(guardrailed, 2)),
            'confidence': int(confidence),
            'justification': justification,
            'formula': formula,
            'trend': str(reg_results.get('trend', 'stable')),
            'volatility': str(reg_results.get('volatility', 'stable')),
            'demand_class': demand_class,
            'trend_strength': float(reg_results.get('trend_strength', 0.0)),
            'safety_stock': float(self.deterministic.compute_safety_stock(series, demand_class)),
        }

    def _log_decision_trace(self, pid: int, decision: Dict):
        logger.info(f"Product {pid}")
        logger.info(f"SES Forecast: {decision['ses_pred']:.2f}")
        logger.info(f"REG Forecast: {decision['reg_pred']:.2f}")
        logger.info(f"Validation WAPE (SES): {decision['wape_ses']:.2f}%")
        logger.info(f"Validation WAPE (REG): {decision['wape_reg']:.2f}%")
        logger.info(f"Validation WAPE (HYBRID): {decision['wape_hybrid']:.2f}%")
        logger.info(f"Selected Model: {decision['selected_model']}")
        logger.info(
            "Adjustment Applied: "
            f"trend_multiplier={decision['adjustment_detail']['trend_multiplier']:.4f}, "
            f"bias_correction={decision['adjustment_detail']['bias_correction']:.4f}, "
            f"safety_factor={decision['adjustment_detail']['safety_factor']:.4f}, "
            f"combined_factor={decision['adjustment_factor']:.4f}"
        )
        logger.info(f"Final Forecast Formula: {decision['formula']}")
        logger.info(f"Final Forecast: {decision['final_forecast']:.2f}")
        logger.info(f"Confidence Score: {decision['confidence']}%")
        logger.info(f"Justification: {decision['justification']}")

    def evaluate_models(self, limit_products=50, test_days=1):
        """
        Leakage-safe evaluation with train/test split + rolling one-step forecast.
        Reports MAE, RMSE, WAP, and Bias for SMA, REG, and HYBRID.
        """
        self.loader.load_and_clean_wrapper()
        all_products = self.loader.demand_history['id_produit'].unique()
        rolling_rows = []

        logger.info("--- MODEL EVALUATION PHASE (rolling backtest, leakage-safe) ---")

        for pid in all_products[:limit_products]:
            history = self.loader.demand_history[
                self.loader.demand_history['id_produit'] == pid
            ].sort_values('date').reset_index(drop=True)

            # Need enough points for robust split + rolling windows
            if len(history) < 35:
                continue

            # Train/Test split: first 80% train, last 20% test
            split_idx = int(len(history) * 0.8)
            split_idx = max(split_idx, 21)
            if split_idx >= len(history) - 5:
                continue

            horizon = max(int(test_days), 1)
            holdout_start = max(split_idx, len(history) - max(horizon * 2, 10))
            test_range = range(holdout_start, len(history) - horizon + 1)
            for t in test_range:
                train_data = history.iloc[:t]
                test_date = history.iloc[t]['date'] if t < len(history) else datetime.now()
                actual_value = float(history.iloc[t:t + horizon]['quantite_demande'].sum())

                # Candidate models: SMA, REG and HYBRID
                sma_pred = self.baseline.predict(train_data, pid) * horizon
                
                reg_results = self.regression.analyze(train_data, pid)
                reg_pred = float(reg_results.get('prediction', 0.0)) * horizon

                deterministic = self.deterministic.predict(train_data, pid, self.regression, target_date=test_date, learning_engine=self.learning_engine)
                hybrid_input = {
                    'id': pid,
                    'sma': deterministic['ses'],
                    'prediction': deterministic['regression'],
                    'yoy_seasonal': deterministic.get('yoy_seasonal', 0.0),
                    'yoy_pattern_detected': deterministic.get('yoy_pattern_detected', False),
                    'trend': deterministic['trend'],
                    'volatility': deterministic['volatility'],
                    'safety_stock': deterministic['safety_stock'],
                    'trend_significant': bool(reg_results.get('trend_significant', False)),
                    'deterministic_base': deterministic['forecast'],
                    'candidates': {
                        'ses': deterministic['ses'],
                        'regression': deterministic['regression']
                    }
                }
                hybrid_out = self.decision_layer.call_mistral_api(hybrid_input)
                hybrid_pred = float(hybrid_out.get('final_forecast', deterministic['forecast'])) * horizon

                rolling_rows.append({
                    'sku_id': pid,
                    'actual': actual_value,
                    'sma': max(0.0, sma_pred),
                    'reg': max(0.0, reg_pred),
                    'hybrid': max(0.0, hybrid_pred)
                })

        if not rolling_rows:
            logger.warning("No products reached evaluation criteria.")
            return None

        eval_df = pd.DataFrame(rolling_rows)
        
        # Calculate Metrics
        metrics = []
        for model in ['sma', 'reg', 'hybrid']:
            total_abs_error = (eval_df[model] - eval_df['actual']).abs().sum()
            total_actual = eval_df['actual'].sum()
            wap = (total_abs_error / total_actual) * 100 if total_actual > 0 else 0
            
            total_forecast = eval_df[model].sum()
            bias_pct = ((total_forecast - total_actual) / total_actual) * 100 if total_actual > 0 else 0
            
            metrics.append({
                'Model': model.upper(),
                'WAP (%)': round(wap, 2),
                'Bias (%)': round(bias_pct, 2)
            })
            
            # --- Model Learning Loop ---
            if model == 'hybrid':
                self.learning_engine.update_global_bias(bias_pct)

        metrics_df = pd.DataFrame(metrics)

        previous_reference = pd.DataFrame([
            {'Model': 'SMA', 'WAP (%)': 52.12, 'Bias (%)': -2.15},
            {'Model': 'REG', 'WAP (%)': 44.59, 'Bias (%)': 3.53},
            {'Model': 'HYBRID', 'WAP (%)': 40.95, 'Bias (%)': 3.98},
        ])
        comparison_df = metrics_df[['Model', 'WAP (%)', 'Bias (%)']].merge(
            previous_reference,
            on='Model',
            suffixes=('_new', '_prev')
        )
        comparison_df['Delta WAP (pp)'] = (comparison_df['WAP (%)_new'] - comparison_df['WAP (%)_prev']).round(2)
        comparison_df['Delta Bias (pp)'] = (comparison_df['Bias (%)_new'] - comparison_df['Bias (%)_prev']).round(2)
        
        # --- Mandatory Final Deliverable: Comparison Table ---
        logger.info("\n" + "="*40)
        logger.info("FINAL MODEL COMPARISON TABLE")
        logger.info("="*40)
        logger.info("\n" + metrics_df.to_string(index=False))
        logger.info("="*40)
        
        # Save to file for deliverable
        metrics_df.to_csv(os.path.join(REPORT_DIR, "model_comparison_results.csv"), index=False)
        with open(os.path.join(REPORT_DIR, "EVALUATION_REPORT.md"), "w", encoding='utf-8') as f:
            f.write("# Model Performance Evaluation\n\n")
            f.write("### Rolling Backtest (Leakage-Safe)\n\n")
            f.write(f"Evaluated points: {len(eval_df)} across up to {limit_products} SKUs.\n\n")
            f.write(metrics_df.to_markdown(index=False))
            f.write("\n\n### Comparison vs Previous Implementation\n\n")
            f.write(comparison_df.to_markdown(index=False))
            f.write("\n\n*Notes:*\n")
            f.write("- Outliers are clipped using IQR; no synthetic zero-filling is introduced in sparse histories.\n")
            f.write("- Regression is used only when trend strength is statistically meaningful.\n")
            f.write("- LLM output is constrained by deterministic guardrails to prevent accuracy degradation.\n")

        return eval_df

    def get_sku_forecast(self, pid):
        """
        Calculates forecast for a specific SKU.
        """
        self.loader.load_and_clean_wrapper()
        current_stock = self.loader.get_current_stock()
        
        history = self.loader.demand_history[self.loader.demand_history['id_produit'] == int(pid)]
        if len(history) < 2:
            return None
        
        decision = self._select_and_compute_forecast(int(pid), history, target_date=None)
        self._log_decision_trace(int(pid), decision)
        
        result = {
            'pid': int(pid),
            'ses': float(decision['ses_pred']),
            'regression': float(decision['reg_pred']),
            'yoy_seasonal': 0.0,
            'yoy_pattern_detected': False,
            'safety_stock': float(decision.get('safety_stock', 0)),
            'trend': str(decision.get('trend', 'unknown')),
            'volatility': str(decision.get('volatility', 'unknown')),
            'demand_class': str(decision.get('demand_class', 'unknown')),
            'trend_strength': float(decision.get('trend_strength', 0.0)),
            'final_forecast': float(decision['final_forecast']),
            'explanation': str(decision.get('justification', "No explanation provided")),
            'current_stock': float(current_stock.get(int(pid), 0))
        }
        return result

    def trigger_daily_preparation(self, target_date=None, limit_products=20):
        """
        REQ 8.1: Trigger preparation one day in advance.
        Analyzes historical stock and delivery data to predict required quantities.
        """
        if target_date is None:
            target_date = datetime.now() + timedelta(days=1)
            
        logger.info(f"--- TRIGGERING PREPARATION FOR {target_date.strftime('%Y-%m-%d')} ---")
        
        self.loader.load_and_clean_wrapper()
        current_stock = self.loader.get_current_stock()
        all_products = self.loader.demand_history['id_produit'].unique()
        
        forecast_metadata = {}
        for pid in all_products[:limit_products]:
            history = self.loader.demand_history[self.loader.demand_history['id_produit'] == pid]
            if len(history) < 2: continue
            
            decision = self._select_and_compute_forecast(int(pid), history, target_date=target_date)
            self._log_decision_trace(int(pid), decision)
            
            forecast_metadata[pid] = {
                'forecast': float(decision['final_forecast']),
                'confidence': int(decision['confidence']),
                'reasoning': f"[{decision['selected_model']}] {decision.get('justification', '')}"
            }
            
        # Generate Orders based on Forecast - Stock
        order = self.order_service.generate_order_advanced(forecast_metadata, current_stock)
        
        if order:
            logger.info(f"SUCCESS: Created Preparation Order {order['order_id']} for tomorrow.")
            # Note: In a real Django app, we would also save to the Transaction model here.
            return order

        return {
            'order_id': f"PREP-{uuid.uuid4().hex[:8].upper()}",
            'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'items': [],
            'status': 'DRAFT',
            'source': 'AI_FORECASTING_SERVICE'
        }

    def run(self, limit_products=10):
        self.loader.load_and_clean_wrapper()
        current_stock = self.loader.get_current_stock()
        
        # Check if we have demand history data
        if self.loader.demand_history is None or self.loader.demand_history.empty:
            logger.warning("No demand history available. Cannot generate forecasts.")
            return {
                'order_id': str(uuid.uuid4())[:8],
                'status': 'DRAFT',
                'items': [],
                'total_items': 0,
                'created_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        
        all_products = self.loader.demand_history['id_produit'].unique()
        
        evaluation_results = []
        forecast_metadata = {}

        logger.info(f"Analyzing {min(len(all_products), limit_products)} products with Statistical & Decision Engine...")
        
        for pid in all_products[:limit_products]:
            history = self.loader.demand_history[self.loader.demand_history['id_produit'] == pid]
            if len(history) < 2: continue
            
            decision = self._select_and_compute_forecast(int(pid), history, target_date=None)
            self._log_decision_trace(int(pid), decision)
            
            # Format combined results for Order Service
            forecast_metadata[pid] = {
                'forecast': float(decision['final_forecast']),
                'safety_stock': float(decision.get('safety_stock', 0)), 
                'confidence': int(decision.get('confidence', 0)),
                'reasoning': f"[{decision['selected_model']}] {decision.get('justification', '')}"
            }

        # Generate Orders
        orders = self.order_service.generate_order_advanced(forecast_metadata, current_stock)
        if orders is not None:
            return orders

        return {
            'order_id': f"PREP-{uuid.uuid4().hex[:8].upper()}",
            'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'items': [],
            'status': 'DRAFT',
            'source': 'AI_FORECASTING_SERVICE'
        }

    def get_high_demand_skus(self, threshold_quantile=0.85):
        """
        Returns a list of SKUs that are predicted to have high demand tomorrow.
        Uses the deterministic forecast engine.
        """
        self.loader.load_and_clean()
        all_products = self.loader.demand_history['id_produit'].unique()
        predictions = {}

        for pid in all_products:
            history = self.loader.demand_history[self.loader.demand_history['id_produit'] == pid]
            if len(history) < 3:
                continue
            
            # Use deterministic predict
            target_date = datetime.now() + timedelta(days=1)
            deterministic = self.deterministic.predict(history, pid, self.regression, target_date=target_date, learning_engine=self.learning_engine)
            predictions[pid] = deterministic['forecast']

        if not predictions:
            return []

        # Find top N%
        pred_values = [v for v in predictions.values() if v > 0]
        if not pred_values: return []
        
        thresh = np.quantile(pred_values, threshold_quantile)
        high_demand_skus = [pid for pid, val in predictions.items() if val >= thresh and val > 0]
        return high_demand_skus

def main():
    # Adjusted path for reorganized structure
    base_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__))) # MOB_AI/backend
    # The file should be in the backend root
    excel_path = os.path.join(base_dir, 'backend', 'WMS_Hackathon_DataPack_Templates_FR_FV_B7_ONLY.xlsx')
    
    # Fallback if not found
    if not os.path.exists(excel_path):
        excel_path = "WMS_Hackathon_DataPack_Templates_FR_FV_B7_ONLY.xlsx"

    service = ForecastingService(excel_path)
    
    # --- REQUIREMENT 8.1: FORECASTING SERVICE ---
    # 1. Prediction & Accuracy Improvement (Mistral Decision Layer)
    logger.info("PHASE 1: Accuracy Evaluation...")
    service.evaluate_models(limit_products=5, test_days=7)
    
    # 2. Trigger Preparation One Day in Advance (Analyzing Stock & Delivery Data)
    logger.info("\nPHASE 2: Triggering Preparation for Tomorrow...")
    tomorrow_order = service.trigger_daily_preparation(limit_products=10)
    
    if tomorrow_order:
        logger.info(f"Order {tomorrow_order['order_id']} generated with {len(tomorrow_order['items'])} SKUs.")
    
    # --- PHASE 4 & 5: GENERATION & INTERACTION DEMO ---
    # 1. AI Generation Phase
    logger.info("\nPHASE 3: Full Run with Supervisor Interaction...")
    order = service.run(limit_products=20)
    
    if order:
        logger.info(f"\n" + "="*50)
        logger.info(f"PHASE 5: SUPERVISOR INTERACTION FLOW")
        logger.info(f"="*50)
        logger.info(f"NEW ORDER ARRIVED FOR REVIEW: {order['order_id']}")
        logger.info(f"Status: {order['status']}")
        
        # Display items for review
        items_df = pd.DataFrame(order['items'])
        logger.info(f"\n--- ITEMS TO REVIEW ---\n{items_df[['sku_id', 'quantity', 'target_forecast', 'confidence', 'current_stock']].to_string(index=False)}")
        
        # 2. Supervisor Actions
        if len(order['items']) > 0:
            # Action A: Supervisor chooses to override one item (the first one)
            sku_to_change = order['items'][0]['sku_id']
            logger.info(f"\n[ACTION] Supervisor is overriding SKU {sku_to_change}...")
            order = service.order_service.validate_order(
                order, 
                sku_id=sku_to_change, 
                override_qty=50.0, 
                justification="Manual buffer added for upcoming promotion detected by local manager.",
                learning_engine=service.learning_engine
            )
            
            # Action B: Supervisor validates the rest of the order (Status moves to VALIDATED)
            logger.info(f"\n--- FINAL VALIDATED ORDER OBJECT ---")
            logger.info(f"Order ID: {order['order_id']}")
            logger.info(f"Final Status: {order['status']}")
            
            final_items_df = pd.DataFrame(order['items'])
            # Show justification for modified items
            logger.info(f"\nFinal Items Sent to Warehouse:\n{final_items_df.to_string(index=False)}")

if __name__ == "__main__":
    main()